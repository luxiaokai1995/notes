*************************************************COURSE*DAY06***************************************
一 : Zookeeper
	1.1 zookeeper介绍
		是一个开源的分布式应用程序协调服务
		保证数据在集群间的事务一致性
		应用场景: 集群分布式锁  集群统一命名服务  分布式协调服务
		Zookeeper角色与特性:
			Leader: 接受所有Follower的提案请求并统一协调发起的提案,负责所有的Follower内部数据交换
			Follower: 直接服务客户端,并参与投票,与Leader进行数据交换
			Observer: 直接服务客户端,不 参与投票,与Leader进行数据交换
		Zookeeper角色与选举:
			角色通过选举产生,选举一个Leader剩下的是Follower(过半数就选举成功)
			Leader死亡重新选举,不够死之前的半数则集群停止工作
			
	1.2 zookeeper安装
				Zookeeper帮助文档:  http://zookeeper.apache.org/doc/r3.4.10/zookeeperAdmin.html
				
		[root@nn01 conf]# tar -xf zookeeper-3.4.13.tar.gz		//解压
		[root@nn01 ~]# mv zookeeper-3.4.13 /usr/local/zookeeper		//移动
		[root@nn01 ~]# cd /usr/local/zookeeper/conf			
		[root@nn01 conf]# mv zoo_sample.cfg zoo.cfg				//修改配置文件名称
		[root@nn01 conf]# vim zoo.cfg			
			server.1=node1:2888:3888				//server.[1,2,3,4]为唯一ID值
			server.2=node2:2888:3888
			server.3=node3:2888:3888
			server.4=nn01:2888:3888:observer
		[root@nn01 conf]# for i in node{1..3};do rsync -aXSH  --delete /usr/local/zookeeper\
							>  ${i}:/usr/local/ &  done			//同步到其他主机
		[root@nn01 conf]# mkdir /tmp/zookeeper			//创建工作目录
		[root@node1 ~]# echo 1 >/tmp/zookeeper/myid		//将本机id重定向到myid文件
		[root@node2 ~]# echo 2 >/tmp/zookeeper/myid
		[root@node3 ~]# echo 3 >/tmp/zookeeper/myid	
		[root@nn01 ~]# echo 4 >/tmp/zookeeper/myid
		
		[root@nn01 conf]# /usr/local/zookeeper/bin/zkServer.sh start		//启动集群
		[root@nn01 conf]# /usr/local/zookeeper/bin/zkServer.sh status		//查看状态

		[root@node2 ~]# socat - TCP:node3:2181		//查看集群服务状态
		ruok		//are you ok		输入
		imok		//i am ok			回复
		[root@node2 ~]# socat - TCP:node3:2181
		conf		//查看配置文件
		[root@node2 ~]# socat - TCP:node3:2181
		status		//查看状态

		[root@nn01 conf]# /root/zkstats nn01 node{1..3}		//用脚本查看主机角色
		[root@nn01 conf]# vim /root/zkstats			//查看集群角色脚本
#!/bin/bash
function getzkstat(){				##定义函数
    exec 2>/dev/null
    exec 8<>/dev/tcp/$1/2181		##打开8号文件描述符
    echo stat >&8						##将stat重定向到8号描述符
    Msg=$(cat <&8 |grep -P "^Mode:")
    echo -e "$1\t${Msg:-Mode: \x1b[31mNULL\x1b[0m}"		##过滤信息
    exec 8<&-		##关闭8号描述符
}

if (( $# == 0 ));then
    echo "${0##*/} zk1 zk2 zk3 ... ..."
else
    for i in $@;do
        getzkstat ${i}
    done
fi

二 : Kafka集群
	2.1 Kafka介绍
		[root@nn01 ~]# tar -xf kafka_2.12-2.1.0.tgz
		[root@nn01 ~]# mv kafka_2.12-2.1.0 /usr/local/kafka
		[root@nn01 ~]# cd /usr/local/kafka/config/
		[root@nn01 config]# vim server.properties
		21 broker.id=60		//serverid 不能重复
		123 zookeeper.connect=node1:2181,node2:2181,node3:2181		//主机地址
		[root@node1 config]# for i in 63 64; do rsync -aSH --delete \
								>  /usr/local/kafka 192.168.1.$i:/usr/local/; done	//拷贝到其他主机
		[root@node2 ~]# vim /usr/local/kafka/config/server.properties     //node2主机修改broker.id
		[root@node3 ~]# vim /usr/local/kafka/config/server.properties     //node3主机修改broker.id
		[root@node1 local]# /usr/local/kafka/bin/kafka-server-start.sh -daemon \
							> /usr/local/kafka/config/server.properties 		//启动集群(每台都做)
		[root@node1 local]# jps        //出现kafka就成功启动
			26483 DataNode
			27859 Jps
			27833 Kafka
		[root@node1 local]# /usr/local/kafka/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1  \
							>  --zookeeper node3:2181 --topic aa 	//创建一个topic(相当于链家)
		[root@node2 ~]# /usr/local/kafka/bin/kafka-console-producer.sh --broker-list \
							> node2:9092 --topic aa        //写一个数据(相当于房主)
		[root@node3 ~]# /usr/local/kafka/bin/kafka-console-consumer.sh \--bootstrap-server 
						> node1:9092 --topic aa        //这边会直接同步(相当于租户)


三 : hadoop高可用
	3.1 环境准备
		1 删除所有主机的/var/local/hadoop/*
			[root@node1 ~]# rm -rf /usr/local/hadoop/*
			
		2 重启所有虚拟机
			
		3 在node1 node2 node3 上启动zookeeper
			[root@node1 ~]# /usr/local/zookeeper/bin/zkServer.sh start
			[root@node1 ~]# jps
				674 QuorumPeerMain
		4 开启新虚拟机nn02 ip192.168.1.66
		
		5 给nn02安装java-1.8.0-openjdk-devel
			[root@nn02 ~]# yum -y install java-1.8.0-openjdk-devel

		6 编辑hosts文件(同步到所有主机)
			[root@nn02 ~]# vim /etc/hosts
				192.168.1.60 nn01
				192.168.1.61 node1
				192.168.1.62 node2
				192.168.1.63 node3
				192.168.1.64 node4
				192.168.1.65 nfsgw
				192.168.1.66 nn02
				
		7 将nn01 上的公钥私钥同步给nn02
			[root@nn01 .ssh]# rm -rf known_hosts 
			[root@nn01 .ssh]# rsync -aXSH /root/.ssh nn02:/root

	3.2 配置hadoop
			修改配置文件:
				hadoop-env.sh
				slave
				core-site.xml
				hdfs-site.xml
				mapred-site.xml
				yarn-site.xml
	3.3 高可用验证
				[root@nn01 ~]# for i in node{1..3}; do rsync -aXSH --delete /usr/local/hadoop \
								>  ${i}:/usr/local/ & done		//将hadoop软件同步到所有node
		步骤一：验证hadoop的高可用		
			1）初始化ZK集群
				[root@nn01 ~]# /usr/local/hadoop/bin/hdfs zkfc -formatZK 		//初始化ZK集群
				18/09/11 15:43:35 INFO ha.ActiveStandbyElector: Successfully created 
					/hadoop-ha/nsdcluster in ZK    //出现Successfully即为成功

			2）在node1，node2，node3上面启动journalnode服务（以node1为例子）
				[root@node1 ~]# /usr/local/hadoop/sbin/hadoop-daemon.sh start journalnode 		//启动journalnode服务
					starting journalnode, logging to /usr/local/hadoop/logs/hadoop-root-journalnode-node1.out
				[root@node1 ~]# jps
					29262 JournalNode

			3）格式化，先在node1，node2，node3上面启动journalnode才能格式化
				[root@nn01 ~]# /usr/local/hadoop//bin/hdfs  namenode  -format  //格式化 
					//出现Successfully即为成功
				[root@nn01 hadoop]# ls /var/hadoop/
					dfs

			4）nn02数据同步到本地 /var/hadoop/dfs
				[root@nn02 ~]# cd /var/hadoop/
				[root@nn02 hadoop]# rsync -aSH  nn01:/var/hadoop/  /var/hadoop/		//同步数据
				[root@nn02 hadoop]# ls
					dfs

			5）初始化 JNS
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs namenode -initializeSharedEdits		初始化JHS
				18/09/11 16:26:15 INFO client.QuorumJournalManager: Successfully started new epoch 1    
				    //出现Successfully，成功开启一个节点

			6）停止 journalnode 服务（node1，node2，node3）
				[root@node1 hadoop]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop journalnode		//停止服务
					stopping journalnode
				[root@node1 hadoop]# jps
					29346 Jps
					26895 QuorumPeerMain

		步骤二：启动集群

			1）nn01上面操作
				[root@nn01 hadoop]# /usr/local/hadoop/sbin/start-all.sh  //启动所有集群

			2）nn02上面操作
				[root@nn02 hadoop]# /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanager	//启动资源管理器?
				starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-root-resourcemanager-nn02.out

			3）查看集群状态
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1	//第一台HDFS
					active			//主
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn2	//第二台HDFS
					standby		//备用
				[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm1	//第一台YARN
					active			//主
				[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm2	//第二台YARN
					standby		//备用
	
			4）查看节点是否加入
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs dfsadmin -report		//查看节点状态
				Live datanodes (3):    //会有三个节点
				[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn  node  -list		//列出节点
				Total Nodes:3

		步骤三：访问集群
			1）查看并创建
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -ls  /		//查看hadoop存储的东西
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -mkdir /aa 		//创建aa
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hadoop  fs -ls  /        //再次查看

			2）验证高可用，关闭 active namenode
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1		//查询是否是主
				active
				[root@nn01 hadoop]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop namenode		//关闭主
				stopping namenode
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1     //查看状态
				//再次查看会报错
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin \
										> -getServiceState nn2  	//nn02由之前的standby变为active
				active
				[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm1
				active
				[root@nn01 hadoop]# /usr/local/hadoop/sbin/yarn-daemon.sh stop resourcemanager  	//停止resourcemanager 
				[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm2
				active

			3） 恢复节点
				[root@nn01 hadoop]# /usr/local/hadoop/sbin/hadoop-daemon.sh start namenode       //启动namenode
				[root@nn01 hadoop]# /usr/local/hadoop/sbin/yarn-daemon.sh start resourcemanager 	//启动resourcemanager
				[root@nn01 hadoop]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState nn1  		//查看
				[root@nn01 hadoop]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState rm1     	//查看









